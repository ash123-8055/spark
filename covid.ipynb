{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8e145c7",
   "metadata": {},
   "source": [
    "<h2>Creating a Spark object</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "107bbd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName(\"covid_analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef03d2ee",
   "metadata": {},
   "source": [
    "<h3>Regional Summary</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b173012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------+----------------+\n",
      "|          WHO Region|total_cases|total_deaths|total_recoveries|\n",
      "+--------------------+-----------+------------+----------------+\n",
      "|            Americas|  8839286.0|    342732.0|       4468616.0|\n",
      "|              Europe|  3299523.0|    211144.0|       1993723.0|\n",
      "|     South-East Asia|  1835297.0|     41349.0|       1156933.0|\n",
      "|Eastern Mediterra...|  1490744.0|     38339.0|       1201400.0|\n",
      "|              Africa|   723207.0|     12223.0|        440645.0|\n",
      "|     Western Pacific|   292428.0|      8249.0|        206770.0|\n",
      "+--------------------+-----------+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df= spark.read.csv(r\"hdfs://localhost:9000/user/hadoop/covid_dataset/archive/country_wise_latest.csv\",header=True)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "result=spark.sql(\"\"\"\n",
    "    SELECT `WHO Region`, \n",
    "        SUM(`Confirmed`) AS total_cases, \n",
    "        SUM(`Deaths`) AS total_deaths, \n",
    "        SUM(`Recovered`) AS total_recoveries \n",
    "    FROM df \n",
    "    GROUP BY `WHO Region` \n",
    "    ORDER BY total_cases DESC;\n",
    "    \"\"\")\n",
    "result.show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32910add",
   "metadata": {},
   "source": [
    "<h3>Top 10 Countries By COVID-19 Recovery Rates</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7f64195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|Country/Region|   recovery_rate|\n",
      "+--------------+----------------+\n",
      "|      Holy See|100.000000000000|\n",
      "|       Grenada|100.000000000000|\n",
      "|      Dominica|100.000000000000|\n",
      "|      Djibouti| 98.379126309547|\n",
      "|       Iceland| 98.327939590076|\n",
      "|        Brunei| 97.872340425532|\n",
      "|   New Zealand| 97.238278741169|\n",
      "|         Qatar| 97.017254121919|\n",
      "|      Malaysia| 96.597035040431|\n",
      "|     Mauritius| 96.511627906977|\n",
      "+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df= spark.read.csv(r\"hdfs://localhost:9000/user/hadoop/covid_dataset/archive/country_wise_latest.csv\",header=True,inferSchema=True)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "result=spark.sql(\"\"\"\n",
    "    select `Country/Region`, \n",
    "    (`Recovered`* 100.0 / `Confirmed`) as recovery_rate \n",
    "    from df\n",
    "    order by recovery_rate \n",
    "    DESC limit 10;\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac04bfef",
   "metadata": {},
   "source": [
    "<h3>Comparison Of Recovery And Fatality Rates By Country</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c05f3a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+---------------+\n",
      "|     Country/Region|     death_rate|  recovery_rate|\n",
      "+-------------------+---------------+---------------+\n",
      "|        Afghanistan| 3.499434685492|69.486804732096|\n",
      "|            Albania| 2.950819672131|56.250000000000|\n",
      "|            Algeria| 4.157580524077|67.339934937261|\n",
      "|            Andorra| 5.733186328556|88.533627342889|\n",
      "|             Angola| 4.315789473684|25.473684210526|\n",
      "|Antigua and Barbuda| 3.488372093023|75.581395348837|\n",
      "|          Argentina| 1.827184976346|43.350097959574|\n",
      "|            Armenia| 1.901577962022|71.315859855576|\n",
      "|          Australia| 1.091289289682|60.844278899562|\n",
      "|            Austria| 3.468236209748|88.753769821967|\n",
      "|         Azerbaijan| 1.389345069960|76.338435262432|\n",
      "|            Bahamas| 2.879581151832|23.821989528796|\n",
      "|            Bahrain| 0.357124765716|91.459399219898|\n",
      "|         Bangladesh| 1.310642059896|55.556636092386|\n",
      "|           Barbados| 6.363636363636|85.454545454545|\n",
      "|            Belarus| 0.799988104266|89.949591827631|\n",
      "|            Belgium|14.785933642440|26.272053953152|\n",
      "|             Belize| 4.166666666667|54.166666666667|\n",
      "|              Benin| 1.977401129944|58.531073446328|\n",
      "|             Bhutan|          0E-12|86.868686868687|\n",
      "+-------------------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df= spark.read.csv(r\"hdfs://localhost:9000/user/hadoop/covid_dataset/archive/country_wise_latest.csv\",header=True,inferSchema=True)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "result=spark.sql(\"\"\"\n",
    "    SELECT `Country/Region`, (`Deaths` * 100.0 / `Confirmed`) AS death_rate, \n",
    "    (`Recovered` * 100.0 / `Confirmed`) AS recovery_rate \n",
    "    FROM df\n",
    "    WHERE `Confirmed` > 0 \n",
    "    ORDER BY \"Country_Region\";\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832e4e4a",
   "metadata": {},
   "source": [
    "<h3>Countries with the Lowest Number of COVID-19 Deaths</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5fcbac2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|     Country/Region|Deaths|\n",
      "+-------------------+------+\n",
      "|            Burundi|     1|\n",
      "|      Liechtenstein|     1|\n",
      "|     Western Sahara|     1|\n",
      "|             Belize|     2|\n",
      "|           Botswana|     2|\n",
      "|             Uganda|     2|\n",
      "|Antigua and Barbuda|     3|\n",
      "|             Brunei|     3|\n",
      "|             Monaco|     4|\n",
      "|             Rwanda|     5|\n",
      "+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df= spark.read.csv(r\"hdfs://localhost:9000/user/hadoop/covid_dataset/archive/country_wise_latest.csv\",header=True,inferSchema=True)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "result=spark.sql(\"\"\"\n",
    "    SELECT `Country/Region`, `Deaths` \n",
    "    FROM df \n",
    "    WHERE `Deaths` > 0 \n",
    "    ORDER BY `Deaths` \n",
    "    ASC LIMIT 10;\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbae1fa",
   "metadata": {},
   "source": [
    "<h3>Countries with the Highest Number of COVID-19 Cases</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6988a1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|Country/Region|Confirmed|\n",
      "+--------------+---------+\n",
      "|            US|  4290259|\n",
      "|        Brazil|  2442375|\n",
      "|         India|  1480073|\n",
      "|        Russia|   816680|\n",
      "|  South Africa|   452529|\n",
      "|        Mexico|   395489|\n",
      "|          Peru|   389717|\n",
      "|         Chile|   347923|\n",
      "|United Kingdom|   301708|\n",
      "|          Iran|   293606|\n",
      "+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df= spark.read.csv(r\"hdfs://localhost:9000/user/hadoop/covid_dataset/archive/country_wise_latest.csv\",header=True,inferSchema=True)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "result=spark.sql(\"\"\"\n",
    "    SELECT `Country/Region`, `Confirmed` \n",
    "    FROM df \n",
    "    ORDER BY `Confirmed` \n",
    "    DESC LIMIT 10;\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db96d84a",
   "metadata": {},
   "source": [
    "<h3>Global Recovery Rate</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2c644e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|    recovery_rate|\n",
      "+-----------------+\n",
      "|62.99339783483538|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df= spark.read.csv(r\"hdfs://localhost:9000/user/hadoop/covid_dataset/archive/worldometer_data.csv\",header=True,inferSchema=True)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "result=spark.sql(\"\"\"\n",
    "SELECT (SUM(`TotalRecovered`)+SUM(`NewRecovered`)) * 100 / SUM(`TotalCases`) as recovery_rate \n",
    "from df;\n",
    "\"\"\")\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57513324",
   "metadata": {},
   "source": [
    "<h3>COVID-19 Trends By Continent</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55bd45ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+-----------+--------------+\n",
      "|        Continent|TotalCases|TotalDeaths|TotalRecovered|\n",
      "+-----------------+----------+-----------+--------------+\n",
      "|           Europe|   2982576|     205232|       1587302|\n",
      "|           Africa|   1011867|      22114|        693620|\n",
      "|             null|       712|         13|           651|\n",
      "|Australia/Oceania|     21735|        281|         12620|\n",
      "|    North America|   5919209|     229855|       3151678|\n",
      "|    South America|   4543273|     154885|       3116150|\n",
      "|             Asia|   4689794|     100627|       3508170|\n",
      "+-----------------+----------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df= spark.read.csv(r\"hdfs://localhost:9000/user/hadoop/covid_dataset/archive/worldometer_data.csv\",header=True,inferSchema=True)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "result=spark.sql(\"\"\"\n",
    "    SELECT `Continent` , SUM(`TotalCases`) as TotalCases, \n",
    "    SUM(`TotalDeaths`) as TotalDeaths, \n",
    "    SUM(`TotalRecovered`) as TotalRecovered \n",
    "    from df \n",
    "    group by `Continent`;\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e2bebd",
   "metadata": {},
   "source": [
    "<h3>Continent That Has The Lowest Cases</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e3854f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|Continent|total_cases|\n",
      "+---------+-----------+\n",
      "|     null|        712|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df= spark.read.csv(r\"hdfs://localhost:9000/user/hadoop/covid_dataset/archive/worldometer_data.csv\",header=True,inferSchema=True)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "result=spark.sql(\"\"\"\n",
    "    SELECT `Continent`, \n",
    "        SUM(`TotalCases`) AS total_cases \n",
    "    FROM df \n",
    "    GROUP BY `Continent` \n",
    "    ORDER BY total_cases \n",
    "    ASC LIMIT 1;\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d40f9",
   "metadata": {},
   "source": [
    "<h3>Recovery Rate by Continent</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "877ef9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|        Continent|    recovery_rate|\n",
      "+-----------------+-----------------+\n",
      "|             null|91.43258426966292|\n",
      "|             Asia|74.80435174764606|\n",
      "|    South America|68.58821822945705|\n",
      "|           Africa|68.54853454060662|\n",
      "|Australia/Oceania|58.06303197607545|\n",
      "|    North America|53.24491836662635|\n",
      "|           Europe|53.21916356867352|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df= spark.read.csv(r\"hdfs://localhost:9000/user/hadoop/covid_dataset/archive/worldometer_data.csv\",header=True,inferSchema=True)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "result=spark.sql(\"\"\"\n",
    "    SELECT `Continent`, \n",
    "        SUM(`TotalRecovered`) * 100.0 / SUM(`TotalCases`) AS recovery_rate \n",
    "    FROM df \n",
    "    WHERE `TotalCases` > 0 \n",
    "    GROUP BY `Continent` \n",
    "    ORDER BY recovery_rate DESC;\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7356f03",
   "metadata": {},
   "source": [
    "<h3>Global Death %</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "66c934fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|global_death_percentage|\n",
      "+-----------------------+\n",
      "|       3.96854825570971|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df= spark.read.csv(r\"hdfs://localhost:9000/user/hadoop/covid_dataset/archive/country_wise_latest.csv\",header=True,inferSchema=True)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "result=spark.sql(\"\"\"\n",
    "    SELECT SUM(`Deaths`) * 100.0 / SUM(`Confirmed`) AS global_death_percentage \n",
    "    FROM df \n",
    "    WHERE `Confirmed` > 0;\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff0b3f6",
   "metadata": {},
   "source": [
    "<h3>Local Death %</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0cd4492e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------+\n",
      "|     Country/Region|death_percentage|\n",
      "+-------------------+----------------+\n",
      "|        Afghanistan|  3.499434685492|\n",
      "|            Albania|  2.950819672131|\n",
      "|            Algeria|  4.157580524077|\n",
      "|            Andorra|  5.733186328556|\n",
      "|             Angola|  4.315789473684|\n",
      "|Antigua and Barbuda|  3.488372093023|\n",
      "|          Argentina|  1.827184976346|\n",
      "|            Armenia|  1.901577962022|\n",
      "|          Australia|  1.091289289682|\n",
      "|            Austria|  3.468236209748|\n",
      "|         Azerbaijan|  1.389345069960|\n",
      "|            Bahamas|  2.879581151832|\n",
      "|            Bahrain|  0.357124765716|\n",
      "|         Bangladesh|  1.310642059896|\n",
      "|           Barbados|  6.363636363636|\n",
      "|            Belarus|  0.799988104266|\n",
      "|            Belgium| 14.785933642440|\n",
      "|             Belize|  4.166666666667|\n",
      "|              Benin|  1.977401129944|\n",
      "|             Bhutan|           0E-12|\n",
      "+-------------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df= spark.read.csv(r\"hdfs://localhost:9000/user/hadoop/covid_dataset/archive/country_wise_latest.csv\",header=True,inferSchema=True)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "result=spark.sql(\"\"\"\n",
    "    SELECT `Country/Region`, \n",
    "        `Deaths` * 100.0 / `Confirmed` AS death_percentage \n",
    "    FROM df \n",
    "    WHERE `Confirmed` > 0 \n",
    "    ORDER BY `Country/Region`;\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c34479",
   "metadata": {},
   "source": [
    "<h3>Global Infection %</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3e0a83c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|  infect_percent|\n",
      "+----------------+\n",
      "|0.29201419840115|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1= spark.read.csv(r\"hdfs://localhost:9000/user/hadoop/covid_dataset/archive/country_wise_latest.csv\",header=True,inferSchema=True)\n",
    "df1.createOrReplaceTempView(\"df1\")\n",
    "df2= spark.read.csv(r\"hdfs://localhost:9000/user/hadoop/covid_dataset/archive/worldometer_data.csv\",header=True,inferSchema=True)\n",
    "df2.createOrReplaceTempView(\"df2\")\n",
    "result=spark.sql(\"\"\"\n",
    "    SELECT SUM(cwl.`Confirmed`)*100.0/ NULLIF(SUM(wd.`Population`), 0) AS infect_percent\n",
    "    from df1 cwl left join df2 wd \n",
    "    on cwl.`Country/Region`=wd.`Country/Region`;\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b3b6e",
   "metadata": {},
   "source": [
    "<h3>Local Infection %</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea27da97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+\n",
      "|     Country/Region|infect_percent|\n",
      "+-------------------+--------------+\n",
      "|        Afghanistan|0.092959533623|\n",
      "|            Albania|0.169593427560|\n",
      "|            Algeria|0.063681987186|\n",
      "|            Andorra|1.173684619167|\n",
      "|             Angola|0.002882605147|\n",
      "|Antigua and Barbuda|0.087746148352|\n",
      "|          Argentina|0.370087382676|\n",
      "|            Armenia|1.261551428212|\n",
      "|          Australia|0.059943912898|\n",
      "|            Austria|0.228128772578|\n",
      "|         Azerbaijan|0.300012524335|\n",
      "|            Bahamas|0.097048900451|\n",
      "|            Bahrain|2.313395274655|\n",
      "|         Bangladesh|0.137229649629|\n",
      "|           Barbados|0.038272717467|\n",
      "|            Belarus|0.711726033260|\n",
      "|            Belgium|0.572915009126|\n",
      "|             Belize|0.012050854606|\n",
      "|              Benin|0.014565532387|\n",
      "|             Bhutan|0.012816479663|\n",
      "+-------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1= spark.read.csv(r\"hdfs://localhost:9000/user/hadoop/covid_dataset/archive/country_wise_latest.csv\",header=True,inferSchema=True)\n",
    "df1.createOrReplaceTempView(\"df1\")\n",
    "df2= spark.read.csv(r\"hdfs://localhost:9000/user/hadoop/covid_dataset/archive/worldometer_data.csv\",header=True,inferSchema=True)\n",
    "df2.createOrReplaceTempView(\"df2\")\n",
    "result=spark.sql(\"\"\"\n",
    "    SELECT cwl.`Country/Region`, (cwl.`Confirmed`*100.0 / wd.`Population`) as infect_percent \n",
    "    from df1 cwl left join df2 wd on cwl.`Country/Region`=wd.`Country/Region` \n",
    "    order by `Country/Region`;\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248ead89",
   "metadata": {},
   "source": [
    "<h3>To find out the countries with the highest infection rates</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a9e5e206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------+\n",
      "|Country/Region|infect_percent|\n",
      "+--------------+--------------+\n",
      "|         Qatar|3.903298127897|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1= spark.read.csv(r\"hdfs://localhost:9000/user/hadoop/covid_dataset/archive/country_wise_latest.csv\",header=True,inferSchema=True)\n",
    "df1.createOrReplaceTempView(\"df1\")\n",
    "df2= spark.read.csv(r\"hdfs://localhost:9000/user/hadoop/covid_dataset/archive/worldometer_data.csv\",header=True,inferSchema=True)\n",
    "df2.createOrReplaceTempView(\"df2\")\n",
    "result=spark.sql(\"\"\"\n",
    "    SELECT cwl.`Country/Region`, \n",
    "        (cwl.`Confirmed`*100.0 / NULLIF(wd.`Population`,0)) as infect_percent \n",
    "    from df1 cwl left join df2 wd on cwl.`Country/Region`=wd.`Country/Region` \n",
    "    where cwl.`Confirmed`>0 AND \n",
    "    wd.`Population` is not null \n",
    "    order by infect_percent \n",
    "    DESC limit 1;\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d681aacd",
   "metadata": {},
   "source": [
    "<h3>To find out the countries and continents with the highest death counts</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9084854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+-----------+\n",
      "|Country/Region|    Continent|TotalDeaths|\n",
      "+--------------+-------------+-----------+\n",
      "|           USA|North America|     162804|\n",
      "|        Brazil|South America|      98644|\n",
      "|        Mexico|North America|      50517|\n",
      "|            UK|       Europe|      46413|\n",
      "|         India|         Asia|      41638|\n",
      "|         Italy|       Europe|      35187|\n",
      "|        France|       Europe|      30312|\n",
      "|         Spain|       Europe|      28500|\n",
      "|          Peru|South America|      20424|\n",
      "|          Iran|         Asia|      17976|\n",
      "|        Russia|       Europe|      14606|\n",
      "|      Colombia|South America|      11939|\n",
      "|         Chile|South America|       9889|\n",
      "|       Belgium|       Europe|       9859|\n",
      "|  South Africa|       Africa|       9604|\n",
      "|       Germany|       Europe|       9252|\n",
      "|        Canada|North America|       8966|\n",
      "|   Netherlands|       Europe|       6153|\n",
      "|      Pakistan|         Asia|       6035|\n",
      "|       Ecuador|South America|       5877|\n",
      "+--------------+-------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df= spark.read.csv(r\"hdfs://localhost:9000/user/hadoop/covid_dataset/archive/worldometer_data.csv\",header=True,inferSchema=True)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "result=spark.sql(\"\"\"\n",
    "    SELECT `Country/Region`, \n",
    "        `Continent`, `TotalDeaths` from df \n",
    "    where `TotalDeaths` is not null \n",
    "    order by `TotalDeaths` DESC;\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da0932c",
   "metadata": {},
   "source": [
    "<h3>Average number of deaths by day (Continents and Countries)(global)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d835fdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-------------+\n",
      "|Country/Region|      Date|average_death|\n",
      "+--------------+----------+-------------+\n",
      "|   Timor-Leste|2020-07-27|          0.0|\n",
      "|         China|2020-07-27|       4656.0|\n",
      "|     Australia|2020-07-27|        167.0|\n",
      "|      Cameroon|2020-07-27|        391.0|\n",
      "|        Greece|2020-07-27|        202.0|\n",
      "|     Greenland|2020-07-27|          0.0|\n",
      "|       Iceland|2020-07-27|         10.0|\n",
      "|      Cambodia|2020-07-27|          0.0|\n",
      "|   Afghanistan|2020-07-27|       1269.0|\n",
      "|       Namibia|2020-07-27|          8.0|\n",
      "|         Haiti|2020-07-27|        158.0|\n",
      "|       Bahrain|2020-07-27|        141.0|\n",
      "|       Ecuador|2020-07-27|       5532.0|\n",
      "|     Mauritius|2020-07-27|         10.0|\n",
      "|          Iraq|2020-07-27|       4458.0|\n",
      "|     Indonesia|2020-07-27|       4838.0|\n",
      "|        Russia|2020-07-27|      13334.0|\n",
      "|      Honduras|2020-07-27|       1166.0|\n",
      "|       Estonia|2020-07-27|         69.0|\n",
      "|        Uganda|2020-07-27|          2.0|\n",
      "+--------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df= spark.read.csv(r\"hdfs://localhost:9000/user/hadoop/covid_dataset/archive/full_grouped.csv\",header=True,inferSchema=True)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "result=spark.sql(\"\"\"\n",
    "    select `Country/Region`, \n",
    "        `Date`,avg(`Deaths`) as average_death \n",
    "    from df \n",
    "    where `Deaths` is not null \n",
    "    group by `Country/Region`,`Date` \n",
    "    order by `Date` desc;\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0732100f",
   "metadata": {},
   "source": [
    "<h3>Average number of deaths by day (Continents and Countries)(local)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1107f27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+----------+-------------+\n",
      "|Country/Region|Continent|      Date|average_death|\n",
      "+--------------+---------+----------+-------------+\n",
      "|            US|     null|2020-07-27|     148011.0|\n",
      "|            US|     null|2020-07-26|     146935.0|\n",
      "|            US|     null|2020-07-25|     146465.0|\n",
      "|            US|     null|2020-07-24|     145560.0|\n",
      "|            US|     null|2020-07-23|     144430.0|\n",
      "|            US|     null|2020-07-22|     143316.0|\n",
      "|            US|     null|2020-07-21|     142121.0|\n",
      "|            US|     null|2020-07-20|     141025.0|\n",
      "|            US|     null|2020-07-19|     140534.0|\n",
      "|            US|     null|2020-07-18|     140119.0|\n",
      "|            US|     null|2020-07-17|     139266.0|\n",
      "|            US|     null|2020-07-16|     138358.0|\n",
      "|            US|     null|2020-07-15|     137415.0|\n",
      "|            US|     null|2020-07-14|     136466.0|\n",
      "|            US|     null|2020-07-13|     135566.0|\n",
      "|            US|     null|2020-07-12|     135205.0|\n",
      "|            US|     null|2020-07-11|     134777.0|\n",
      "|            US|     null|2020-07-10|     134101.0|\n",
      "|            US|     null|2020-07-09|     133290.0|\n",
      "|            US|     null|2020-07-08|     132300.0|\n",
      "+--------------+---------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1= spark.read.csv(r\"hdfs://localhost:9000/user/hadoop/covid_dataset/archive/full_grouped.csv\",header=True,inferSchema=True)\n",
    "df1.createOrReplaceTempView(\"df1\")\n",
    "df2= spark.read.csv(r\"hdfs://localhost:9000/user/hadoop/covid_dataset/archive/worldometer_data.csv\",header=True,inferSchema=True)\n",
    "df2.createOrReplaceTempView(\"df2\")\n",
    "result=spark.sql(\"\"\"\n",
    "    SELECT fg.`Country/Region`, \n",
    "        wd.`Continent`, \n",
    "        fg.`Date`, \n",
    "        avg(fg.`Deaths`) as average_death \n",
    "    from df1 fg left join df2 wd on fg.`Country/Region`=wd.`Country/Region` \n",
    "    where fg.`Deaths` is not null \n",
    "    group by fg.`Country/Region`, wd.`Continent`,fg.`Date` \n",
    "    order by average_death DESC;\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fe0c9a",
   "metadata": {},
   "source": [
    "<h3>Average of cases divided by the number of population of each country (TOP 10)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "22885ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+\n",
      "|Country/Region|      average_case|\n",
      "+--------------+------------------+\n",
      "|         Qatar|3.9921575750450000|\n",
      "| French Guiana|2.7145648579590000|\n",
      "|       Bahrain|2.5130239079750000|\n",
      "|    San Marino|2.0596381637100000|\n",
      "|         Chile|1.9164810228280000|\n",
      "|        Panama|1.6527039892330000|\n",
      "|        Kuwait|1.6378443167540000|\n",
      "|          Oman|1.5769043963730000|\n",
      "|           USA|1.5193862960520000|\n",
      "|  Vatican City|1.4981273408240000|\n",
      "+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df= spark.read.csv(r\"hdfs://localhost:9000/user/hadoop/covid_dataset/archive/worldometer_data.csv\",header=True,inferSchema=True)\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "result=spark.sql(\"\"\"\n",
    "    select `Country/Region`, \n",
    "        avg(`TotalCases`* 100.0 /NULLIF(`Population`,0)) as average_case \n",
    "    from df \n",
    "    where `Population` is not null \n",
    "    group by `Country/Region` \n",
    "    order by average_case DESC limit 10;\n",
    "\"\"\")\n",
    "result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
